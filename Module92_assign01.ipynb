{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## TOPIC: Understanding Pooling and Padding in CNN\n",
        "1. Describe the purpose and benefits of pooling in CNN\n",
        "2. Explain the difference between min pooling and max pooling.\n",
        "3. Discuss the concept of padding in CNN and its significance.\n",
        "4. Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size."
      ],
      "metadata": {
        "id": "D2faXPPljHEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solutions:\n",
        "\n",
        "###1.\n",
        "Pooling in Convolutional Neural Networks (CNNs) serves several purposes and offers multiple benefits:\n",
        "\n",
        "- **Dimensionality Reduction**: Pooling layers are used to reduce the dimensions of the feature maps, thus decreasing the number of parameters to learn and the amount of computation performed in the network. This leads to a significant reduction in the spatial dimensions of the input.\n",
        "\n",
        "- **Robustness to Feature Position Variations**: Pooling makes the model more robust to variations in the position of the features in the input image. It helps the model to detect an object in an image regardless of its position. This property is referred to as \"local translation invariance\".\n",
        "\n",
        "- **Feature Summarization**: The pooling layer summarizes the features present in a region of the feature map generated by a convolution layer. Two common types of pooling are:\n",
        "    - **Max Pooling**: This operation selects the maximum element from the region of the feature map covered by the filter. Thus, the output after max-pooling layer would be a feature map containing the most prominent features of the previous feature map.\n",
        "    - **Average Pooling**: This operation computes the average of the elements present in the region of feature map covered by the filter. Thus, while max pooling gives the most prominent feature in a particular patch of the feature map, average pooling gives the average of features present in a patch.\n",
        "\n",
        "- **Overfitting Control**: The consequence of adding pooling layers is the reduction of overfitting, increased efficiency, and faster training times in a CNN model.\n",
        "\n",
        "- **Global Pooling**: Global pooling reduces each channel in the feature map to a single value. This is equivalent to using a filter of dimensions equal to the dimensions of the feature map.\n",
        "\n",
        "In summary, pooling layers play a crucial role in CNN architectures by summarizing and consolidating the feature maps, making the model more efficient and robust.\n",
        "\n"
      ],
      "metadata": {
        "id": "1L8MBVO-lWOi"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jivTg4F0rIOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2\n",
        "Both min pooling and max pooling are techniques used in pooling layers of Convolutional Neural Networks (CNNs) to process data, but they go about it in opposite ways:\n",
        "\n",
        "**Max Pooling:**\n",
        "\n",
        "* This is the most commonly used pooling technique.\n",
        "* In max pooling, for a specific region in the feature map, the operation finds the **maximum** value among the elements within that region.\n",
        "* This essentially captures the most prominent feature within that local area.\n",
        "* Max pooling is particularly useful for identifying sharp features like edges and corners in images, which are often crucial for object recognition.\n",
        "* It works well when the features of interest have higher intensity values compared to the background (e.g., white digits on a black background).\n",
        "\n",
        "**Min Pooling:**\n",
        "\n",
        "* Min pooling, on the other hand, takes the **minimum** value within the defined region of the feature map.\n",
        "* This captures the least prominent feature in that local area.\n",
        "* While less common than max pooling, it can be useful in specific scenarios where the background information is important.\n",
        "* For instance, if the features of interest have lower intensity values compared to the background (e.g., dark text on a light background), min pooling might be a better choice.\n",
        "\n",
        "Here's a table summarizing the key differences:\n",
        "\n",
        "| Feature                 | Max Pooling | Min Pooling |\n",
        "|-------------------------|--------------|------------|\n",
        "| Operation               | Finds maximum value | Finds minimum value |\n",
        "| Captures                 | Most prominent feature | Least prominent feature |\n",
        "| Useful for              | Edges, corners, high-intensity features | Background information, low-intensity features |\n",
        "\n",
        "**Additional Points:**\n",
        "\n",
        "* Min pooling is generally less popular than max pooling because it can lead to a lot of zero activations in the network, especially with ReLU activation functions. This can hinder learning in subsequent layers.\n",
        "* The choice between min pooling and max pooling depends on the specific application and the nature of the data being processed.\n",
        "\n",
        "In conclusion, both max pooling and min pooling offer ways to summarize local information in CNNs, but they target opposite ends of the value spectrum within a defined region. The best choice depends on the characteristics of your data and the features you want to emphasize."
      ],
      "metadata": {
        "id": "kW1AZwzxoOnC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xj_S_KA7qv9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3\n",
        "In Convolutional Neural Networks (CNNs), padding refers to the technique of adding extra elements, typically zeros, around the borders of the input data (usually images) before performing a convolution operation. This seemingly simple addition has significant consequences for how CNNs process information and ultimately affects their performance.\n",
        "\n",
        "### Significance of Padding in CNNs:\n",
        "\n",
        "1. **Preserves Spatial Information:**\n",
        "\n",
        "   * Standard convolution with no padding shrinks the output feature map compared to the input. This is because the filter \"slides\" across the image, and at the edges, it can't capture the full influence of the filter due to a lack of surrounding pixels.\n",
        "   * Padding adds a buffer of extra pixels around the borders, allowing the filter to operate on the entire image without losing information from the edges. This is crucial for tasks where retaining spatial details is important, such as object recognition in images.\n",
        "\n",
        "2. **Controls Output Size:**\n",
        "\n",
        "   * With padding, you can control the output size of the convolution operation. There are two main approaches:\n",
        "      * **Same Padding:** This padding strategy adds enough zeros to maintain the same spatial dimensions (height and width) for the output feature map as the input.\n",
        "      * **Valid Padding:** This approach performs convolution only on areas where the filter entirely fits within the image boundaries. The resulting output size is smaller than the input.\n",
        "   * Choosing the right padding strategy depends on your network architecture and desired output size.\n",
        "\n",
        "3. **Reduces Information Loss:**\n",
        "\n",
        "   * Without padding, valuable information from the image edges gets discarded during convolution. Padding mitigates this issue by creating a \"buffer zone\" around the image, allowing the filter to capture features even at the borders. This can improve the model's ability to learn and recognize objects in various positions within the image.\n",
        "\n",
        "4. **Improves Model Efficiency (Optional):**\n",
        "\n",
        "   * In some cases, using specific padding strategies (like same padding) can improve computational efficiency. This is because maintaining a consistent output size throughout the network allows for efficient reuse of filters and memory allocation.\n",
        "\n",
        "###  Trade-offs to Consider:\n",
        "\n",
        "* **Increased Computational Cost:** Adding padding increases the input size, leading to slightly more computations during convolution. However, the benefits of preserving information often outweigh this cost.\n",
        "* **Potential for Overfitting (for Same Padding):** If the padding is excessive, it might introduce redundant information and lead to overfitting, especially with small datasets.\n",
        "\n",
        "Overall, padding is a crucial technique in CNNs that helps maintain spatial information, control output size, and reduce information loss. Understanding the trade-offs and choosing the right padding strategy can significantly impact the performance of your CNN model."
      ],
      "metadata": {
        "id": "Ovaan4NnrJm6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9iCC-BBht_8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4\n",
        "Here's a comparison of zero-padding and valid-padding in CNNs, focusing on their effects on the output feature map size:\n",
        "\n",
        "**Effect on Output Feature Map Size:**\n",
        "\n",
        "| Padding Type | Output Feature Map Size | Description |\n",
        "|---|---|---|\n",
        "| Zero-Padding | **Can be same size or smaller than input** |  Zero-padding adds extra zeros around the borders of the input feature map. The amount of padding added determines the final size of the output.  \n",
        "    * **Same Padding:** This strategy adds enough zeros to maintain the **same spatial dimensions (height and width)**  for the output feature map as the input.\n",
        "    * **Custom Padding:**  You can choose a specific padding value to achieve a desired output size that is still larger than with valid padding. |\n",
        "| Valid-Padding | **Always smaller than input** | Valid-padding essentially means **no padding** is added. The convolution operation only considers pixels where the filter entirely overlaps with the input. This reduces the output size because some border pixels cannot be included in the convolution due to a lack of surrounding data. |\n",
        "\n",
        "Here's an analogy: Imagine a filter as a small stamp placed on an image.\n",
        "\n",
        "* **Zero-padding** adds a border around the image like a frame for the stamp. This allows the stamp to fully \"touch\" all areas of the image, potentially resulting in the same size output as the original image (same padding) or a slightly smaller size (custom padding).\n",
        "* **Valid-padding** is like using the stamp directly on the image without any additional frame. The output size reflects the areas where the entire stamp fit on the image, leading to a smaller output.\n",
        "\n",
        "**Additional Points:**\n",
        "\n",
        "* Choosing between zero-padding and valid-padding depends on your network architecture and goals.\n",
        "* Same padding is often preferred when preserving spatial information is crucial, such as for object localization tasks.\n",
        "* Valid-padding can be used when you want to progressively shrink the feature maps to capture higher-level features or reduce computational cost (although zero-padding with efficient filter reuse can also achieve this in some cases).\n",
        "\n",
        "\n",
        "In conclusion, zero-padding offers more control over the output size, allowing you to potentially maintain or slightly reduce the dimensions from the input. Valid-padding always results in a smaller output size compared to the input. The choice between them depends on your specific needs for spatial information preservation and network architecture design.\n"
      ],
      "metadata": {
        "id": "ejTzWKMqv-kP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x8HSu688nrhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TOPIC: Exploring LeNet\n",
        "1. provide a brief overview of LeNet-5 architecture.\n",
        "2. Describe the key components of LeNet-5 and their respective purposes.\n",
        "3. Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks.\n",
        "4. Implement LeNet-5 using a deep learning framework of your choice(e.g,Tensorflow, PyTorch) and train it on a publicly available dataset(e.g, MNIST). Evaluate its performance and provide insights."
      ],
      "metadata": {
        "id": "nNTkojRHnsgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solutions"
      ],
      "metadata": {
        "id": "C0XPCECuosNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1\n",
        "LeNet-5 is a pioneering convolutional neural network (CNN) architecture proposed by Yann LeCun and others in 1998. It was designed for recognizing handwritten and machine-printed characters.\n",
        "\n",
        "Here's a brief overview of the LeNet-5 architecture:\n",
        "\n",
        "- **Input Layer**: The input to this model is a 32x32 grayscale image.\n",
        "\n",
        "- **C1 - Convolutional Layer**: This layer performs the first convolution operation using 6 filters of size 5x5, resulting in 6 feature maps of size 28x28.\n",
        "\n",
        "- **S2 - Pooling Layer (Subsampling Layer)**: This layer performs average pooling, reducing the size of each feature map by half to 14x14.\n",
        "\n",
        "- **C3 - Convolutional Layer**: This layer applies 16 filters of size 5x5, resulting in 16 feature maps of size 10x10.\n",
        "\n",
        "- **S4 - Pooling Layer (Subsampling Layer)**: This layer performs another round of average pooling, further reducing the size of each feature map by half to 5x5.\n",
        "\n",
        "- **C5 - Convolutional Layer**: This layer is often referred to as a flattening layer, transforming the 3D input to a 1D vector.\n",
        "\n",
        "- **F6 - Fully Connected Layer**: This layer is a fully connected layer.\n",
        "\n",
        "- **Output Layer**: The final layer is a softmax classifier that outputs the classification results.\n",
        "\n",
        "The architecture of LeNet-5 is simple and straightforward, making it a popular choice for image classification tasks. It's worth noting that the LeNet-5 architecture is considered small and efficient by modern standards.\n",
        "\n"
      ],
      "metadata": {
        "id": "0SbL7C3-owQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2\n",
        "LeNet-5, a foundational convolutional neural network (CNN),  played a crucial role in shaping modern CNN architectures. Here's a breakdown of its key components and their purposes:\n",
        "\n",
        "**1. Convolutional Layers (C1 and C3):**\n",
        "\n",
        "* There are two convolutional layers in LeNet-5, typically named C1 and C3.\n",
        "* These layers apply multiple filters (small learnable kernels) to the input image or the previous feature maps.\n",
        "* The filters scan the image, detecting specific patterns and generating feature maps. For instance, early convolutional layers might capture edges and lines, while later ones might learn more complex shapes or object parts.\n",
        "\n",
        "**2. Pooling Layers (S2 and S4):**\n",
        "\n",
        "* LeNet-5 utilizes two pooling layers, S2 and S4, typically using average pooling.\n",
        "* Pooling layers downsample the feature maps by summarizing information from a local region (e.g., averaging pixel values within a 2x2 area).\n",
        "* This reduces the dimensionality of the data, making the network more computationally efficient and introducing some translation invariance.\n",
        "* Translation invariance means the network becomes less sensitive to small shifts of the features within the image.\n",
        "\n",
        "**3. Flattening Layer:**\n",
        "\n",
        "* After the second pooling layer, the data is in the form of multiple feature maps.\n",
        "* The flattening layer transforms these feature maps into a single long vector. This allows the network to feed the extracted features into fully-connected layers.\n",
        "\n",
        "**4. Fully-Connected Layers (F6 and Output):**\n",
        "\n",
        "* LeNet-5 has two fully-connected layers, F6 and the output layer.\n",
        "* Unlike convolutional layers that operate locally on feature maps, fully-connected layers connect every neuron from the previous layer to all neurons in the current layer. This allows them to learn more complex relationships between the extracted features.\n",
        "* These layers typically use a non-linear activation function (like ReLU) to introduce non-linearity and improve the model's ability to learn complex patterns.\n",
        "\n",
        "**5. Output Layer:**\n",
        "\n",
        "* The final layer uses a function like softmax to convert the activations from the last fully-connected layer into class probabilities.\n",
        "* In LeNet-5's case of character recognition, the softmax function might output probabilities for each possible digit (0-9). The network predicts the class with the highest probability as the recognized character.\n",
        "\n",
        "**Overall Significance:**\n",
        "\n",
        "These components working together achieve feature extraction, dimensionality reduction, and classification. Although a relatively small network by today's standards, LeNet-5 laid the groundwork for the development of powerful CNNs used extensively in image recognition, computer vision, and various other deep learning applications.  "
      ],
      "metadata": {
        "id": "Q2uamhZ9sElN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PkhniS4qxH_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3\n",
        "## Advantages of LeNet-5 for Image Classification:\n",
        "\n",
        "* **Pioneering Architecture:** LeNet-5 established the fundamental building blocks of convolutional neural networks (CNNs) used in image classification today. It introduced the concept of using convolutional layers for feature extraction, pooling layers for dimensionality reduction and translation invariance, and fully-connected layers for classification.\n",
        "* **Efficiency:** Compared to modern CNNs, LeNet-5 is a relatively small and computationally efficient architecture. This makes it faster to train and requires less computational resources, which can be beneficial for applications with limited processing power.\n",
        "* **Effective for Simple Image Classification:** LeNet-5 was particularly successful in tasks like handwritten digit recognition (MNIST dataset) due to its ability to capture basic features like edges and lines relevant for these tasks.\n",
        "\n",
        "## Limitations of LeNet-5 for Image Classification:\n",
        "\n",
        "* **Limited Complexity:**  LeNet-5 has a shallow architecture with a small number of layers and filters. This limits its ability to learn complex features and patterns required for classifying more intricate images compared to modern CNNs with deeper architectures.\n",
        "* **Limited Performance on Large Datasets:** While effective for small datasets like MNIST, LeNet-5 struggles with larger and more diverse image datasets commonly used in modern computer vision tasks. These datasets often contain more complex objects and require deeper networks to capture the necessary features for accurate classification.\n",
        "* **Limited Color Image Processing:** The original LeNet-5 was designed for grayscale images. Processing color images requires additional modifications to handle multiple color channels, further increasing the network complexity and potentially computational cost.\n",
        "\n",
        "## Overall:\n",
        "\n",
        "LeNet-5, though a historical landmark, has limitations in handling complex image classification tasks compared to more advanced CNN architectures. However, its core principles and design continue to be the foundation for modern CNNs. LeNet-5's efficiency and effectiveness for simpler tasks still make it a relevant model for specific applications with limited computational resources or well-defined feature sets."
      ],
      "metadata": {
        "id": "9aNa5B-rzMaR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-QkEEBoFzcdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4\n"
      ],
      "metadata": {
        "id": "yghPU7kI02HQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.datasets import Mnist\n",
        "from keras.layers import Conv2D,MaxPooling2D,AveragePooling2D\n",
        "from keras.layers import Dense,Flatten\n",
        "from keras.models import Sequential\n",
        "\n",
        "# load the Mnist dataset\n",
        "(X_train, y_train),(X_test, y_test) = Mnist.load_data()\n",
        "\n",
        "# Normalize pixel values between 0 and 1\n",
        "X_train = X_train/255.0\n",
        "X_test = X_test/255.0\n",
        "\n",
        "# convert labels to one-hot encoding\n",
        "y_train = keras.utils.to_categorical(y_train, 10)\n",
        "y_test= keras.utils.to_categorical(y_test,10)\n",
        "\n",
        "# Building the model architecture\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(6, kernel_size=(5,5), padding='valid', activation='tanh', input_shape=(32,32,3)))\n",
        "model.add(AveragePooling2D(pool_size=(2,2), strides=2, padding='valid'))\n",
        "\n",
        "model.add(Conv2D(16, kernel_size=(5,5), padding='valid', activation='tanh'))\n",
        "model.add(AveragePooling2D(pool_size=(2,2), strides=2, padding='valid'))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(120, activation='tanh'))\n",
        "model.add(Dense(84, activation='tanh'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    loss= keras.metrics.categorical_crossentropy,\n",
        "    optimizer= keras.optimizers.Adam(),\n",
        "    metrics= [\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=128, epochs=2, validation_data=(X_test,y_test))\n",
        "score= model.evaluate(X_test,y_test)\n",
        "\n",
        "print(\"Test Loss: \", score[0])\n",
        "print(\"Test Accuracy: \", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lt-We9IUxq3V",
        "outputId": "bf4d1f6b-b348-44b1-ede5-1bd4ac24212d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 6s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 6)         456       \n",
            "                                                                 \n",
            " average_pooling2d (Average  (None, 14, 14, 6)         0         \n",
            " Pooling2D)                                                      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " average_pooling2d_1 (Avera  (None, 5, 5, 16)          0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 400)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 120)               48120     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 62006 (242.21 KB)\n",
            "Trainable params: 62006 (242.21 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/2\n",
            "391/391 [==============================] - 7s 7ms/step - loss: 1.8404 - accuracy: 0.3514 - val_loss: 1.7287 - val_accuracy: 0.3922\n",
            "Epoch 2/2\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 1.6899 - accuracy: 0.4074 - val_loss: 1.6349 - val_accuracy: 0.4195\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 1.6349 - accuracy: 0.4195\n",
            "Test Loss:  1.6348565816879272\n",
            "Test Accuracy:  0.4194999933242798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bMJ8rAzn1Mmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TOPIC: Analyzing Alexnet\n",
        "1. Present an overview  of the AlexNet architecture.\n",
        "2. Explain the architectural innovations introduced in AlexNet that contributed to its brekthrough performance.\n",
        "3. Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet.\n",
        "4. Implement AlexNet using a deep learning framework of your choice and evaluate its performance on a dataset of your choice."
      ],
      "metadata": {
        "id": "Bav4PWcR2QDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solutions:"
      ],
      "metadata": {
        "id": "cONCCJgg3Jx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1\n",
        "AlexNet is a significant convolutional neural network (CNN) architecture that won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. It was proposed by Alex Krizhevsky and his colleagues.\n",
        "\n",
        "Here's a brief overview of the AlexNet architecture:\n",
        "\n",
        "1. **Input Layer**: The input to AlexNet is an RGB image of size 227x227x3.\n",
        "\n",
        "2. **Convolutional and Max Pooling Layers**: AlexNet has five convolutional layers. The first, second, and fifth convolutional layers are followed by max pooling layers. The first convolutional layer uses 96 filters of size 11x11 with a stride of 4. The second convolutional layer applies 256 filters of size 5x5⁶ The third, fourth, and fifth convolutional layers apply 384, 384, and 256 filters of size 3x3, respectively.\n",
        "\n",
        "3. **Fully Connected Layers**: After the convolutional and max pooling layers, AlexNet has three fully connected layers. The first two fully connected layers apply a dropout of 0.5 to prevent overfitting.\n",
        "\n",
        "4. **Output Layer**: The final layer is a softmax classifier that outputs the classification results.\n",
        "\n"
      ],
      "metadata": {
        "id": "2XIP4ve_5Epl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2\n",
        "AlexNet introduced several key architectural innovations that significantly contributed to its breakthrough performance in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. Here are the main ones:\n",
        "\n",
        "1. **ReLU Activation Function**: AlexNet was one of the first deep learning models to use the Rectified Linear Unit (ReLU) as an activation function. This was a significant innovation because ReLU helps to mitigate the vanishing gradient problem, allowing models to learn faster and deeper.\n",
        "\n",
        "2. **Use of Dropout**: AlexNet introduced the use of dropout layers, which randomly set a proportion of input neurons to 0 at each update during training time. This prevents units from co-adapting too much to the data and acts as a form of regularization, reducing overfitting.\n",
        "\n",
        "3. **Data Augmentation**: To further combat overfitting, AlexNet implemented data augmentation techniques such as image translations, horizontal reflections, and patch extractions. This effectively increased the size of the training set.\n",
        "\n",
        "4. **Multiple GPUs**: AlexNet was trained on two GPUs due to the limited memory available on a single GPU. This allowed the network to be larger and more powerful.\n",
        "\n",
        "5. **Local Response Normalization (LRN)**: After the ReLU activation function in the first and second convolutional layers, a Local Response Normalization (LRN) operation was performed. This operation encourages lateral inhibition, a concept inspired by the biological processes observed in a neuron's response.\n",
        "\n",
        "6. **Overlapping Pooling**: AlexNet used overlapping pooling, where the pooling windows were overlapping, unlike traditional CNNs that used non-overlapping pooling windows. This reduced the top-1 and top-5 error rates.\n",
        "\n",
        "These innovations helped AlexNet achieve state-of-the-art performance and spurred further research and development in deep learning.\n"
      ],
      "metadata": {
        "id": "BvkDWodW5hb5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tBFXyXdk8xI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3\n",
        "**Convolutional Layers in AlexNet**\n",
        "* Convolutional layers are the foundation of AlexNet's architecture.\n",
        "* They apply multiple filters of varying sizes to the input image, scanning it like a sliding window.\n",
        "* Each filter detects specific features in the image, such as edges, lines, or shapes.\n",
        "* The output of a convolutional layer is a feature map that represents the presence and location of these features within the image.\n",
        "* AlexNet uses five convolutional layers stacked together, allowing it to extract features at different levels of complexity, from basic edges to more intricate object parts.\n",
        "\n",
        "**Pooling Layers in AlexNet**\n",
        "* Pooling layers are another crucial component in AlexNet.\n",
        "* They serve two main purposes: dimensionality reduction and introducing translation invariance.\n",
        "* Pooling layers downsample the feature maps generated by convolutional layers.\n",
        "* A common pooling operation is max pooling, which selects the maximum value from a predefined region (e.g., 2x2 window) in the feature map.\n",
        "* This reduces the number of parameters in the network and makes it more computationally efficient.\n",
        "* Additionally, pooling introduces translation invariance. By selecting the most prominent feature within a local region, pooling makes the network less sensitive to small shifts of the object within the image.\n",
        "\n",
        "**Fully Connected Layers in AlexNet**\n",
        "* Fully connected layers act as the final decision-making stage in AlexNet.\n",
        "* Unlike convolutional layers that operate locally on features, fully connected layers connect every neuron from the previous layer to all neurons in the current layer.\n",
        "* This allows them to learn complex relationships between the extracted features.\n",
        "* AlexNet uses three fully connected layers with a ReLU (Rectified Linear Unit) activation function in the hidden layers.\n",
        "* The final layer employs a softmax activation function to predict the probability of the input image belonging to a specific class (e.g., dog, cat, airplane).\n"
      ],
      "metadata": {
        "id": "FMQ8pJhw9HZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4"
      ],
      "metadata": {
        "id": "gF-SWg789ux_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
        "from keras.layers import BatchNormalization"
      ],
      "metadata": {
        "id": "1Q4PPu7WuUDN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "6f6b_8kr9x5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Data\n",
        "from keras.datasets import  cifar10\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "(X_train,y_train),(X_test,y_test) = cifar10.load_data()\n",
        "\n",
        "# scaling the train and test data\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# encoding the data\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)"
      ],
      "metadata": {
        "id": "BxOVzfqXIImz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CJOPSLhA-L-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHBWSBPZOtYF",
        "outputId": "ca76bfdd-76f7-458d-8221-c12667c0e763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3) (10000, 32, 32, 3)\n",
            "(50000, 10) (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape,X_test.shape)\n",
        "print(y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bY2esTU7-wk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvQrF5R4PeBP",
        "outputId": "e2972d7c-66e8-4287-8d9a-5436975bc2db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 96)          34944     \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 8, 96)          0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 4, 4, 96)          0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 4, 4, 96)          384       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 4, 4, 256)         614656    \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 4, 4, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 2, 2, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 2, 2, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 2, 2, 384)         885120    \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 2, 2, 384)         0         \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 2, 2, 384)         1536      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 2, 2, 384)         1327488   \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 2, 2, 384)         0         \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 2, 2, 384)         1536      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 2, 2, 256)         884992    \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 2, 2, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 1, 1, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 1, 1, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4096)              1052672   \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 4096)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4096)              0         \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 4096)              16384     \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4096)              16781312  \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 4096)              0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 4096)              0         \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 4096)              16384     \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                40970     \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 21660426 (82.63 MB)\n",
            "Trainable params: 21641290 (82.55 MB)\n",
            "Non-trainable params: 19136 (74.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# creating a Sequential  model\n",
        "model = Sequential()\n",
        "\n",
        "# 1st Conv layer\n",
        "model.add(Conv2D(filters=96, input_shape=(32,32,3), kernel_size=(11,11), strides=(4,4), padding= 'same'))\n",
        "model.add(Activation('relu'))\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same'))\n",
        "# Batch Normalization before passing it to the next layer\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 2nd Convulational Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 3rd Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 4th Convolutional Layer\n",
        "model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 5th Convolutional Layer\n",
        "model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "# Pooling\n",
        "model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='same'))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Passing it to a dense layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# 1st Dense Layer\n",
        "model.add(Dense(4096, input_shape=(32*32*3,)))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout to prevent overfitting\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 2nd Dense Layer\n",
        "model.add(Dense(4096))\n",
        "model.add(Activation('relu'))\n",
        "# Add Dropout\n",
        "model.add(Dropout(0.4))\n",
        "# Batch Normalisation\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MMSou5UCGkQs"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IohC8VHm-0Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8Brj24fGpM9",
        "outputId": "a7407151-2bda-4697-e82e-cfe7bec3489f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 23s 18ms/step - loss: 2.0721 - accuracy: 0.3394 - val_loss: 2.8403 - val_accuracy: 0.3785\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 13s 17ms/step - loss: 1.6710 - accuracy: 0.4529 - val_loss: 1.8036 - val_accuracy: 0.4127\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 13s 17ms/step - loss: 1.5033 - accuracy: 0.5065 - val_loss: 1.8955 - val_accuracy: 0.4408\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 13s 17ms/step - loss: 1.3989 - accuracy: 0.5388 - val_loss: 1.6671 - val_accuracy: 0.4548\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 13s 17ms/step - loss: 1.4217 - accuracy: 0.5341 - val_loss: 2.5566 - val_accuracy: 0.2629\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e38ec434a60>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Train\n",
        "model.fit(X_train, y_train, batch_size=64, epochs=5, verbose=1,validation_data=(X_test,y_test), shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXIK_hEw-7Bg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}